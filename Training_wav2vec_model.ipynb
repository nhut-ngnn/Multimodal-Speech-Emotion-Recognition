{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch_geometric.data import Data\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.metrics import confusion_matrix, balanced_accuracy_score, accuracy_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Customized_Dataset(Dataset):\n",
    "    def __init__(self, metadata):\n",
    "        super(Customized_Dataset, self).__init__()\n",
    "        self.data = pickle.load(open(metadata, 'rb'))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        with torch.no_grad():\n",
    "            text_embed = self.data[idx]['text_embed']\n",
    "            audio_embed = self.data[idx]['audio_embed']\n",
    "            label = self.data[idx]['label']\n",
    "\n",
    "        return text_embed, audio_embed, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "train_metadata = \"C:/Users/admin/Documents/Speech-Emotion_Recognition-2/features/IEMOCAP_BERT_wav2vec_train.pkl\"\n",
    "val_metadata = \"C:/Users/admin/Documents/Speech-Emotion_Recognition-2/features/IEMOCAP_BERT_wav2vec_val.pkl\"\n",
    "test_metadata = \"C:/Users/admin/Documents/Speech-Emotion_Recognition-2/features/IEMOCAP_BERT_wav2vec_test.pkl\"\n",
    "train_dataset = Customized_Dataset(train_metadata)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataset = Customized_Dataset(val_metadata)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridAttentionMMSER(nn.Module):\n",
    "    def __init__(self, num_classes=4, text_dim=768, audio_dim=768):\n",
    "        super(HybridAttentionMMSER, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Feature extraction layers\n",
    "        self.text_fc = nn.Sequential(\n",
    "            nn.Linear(text_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.BatchNorm1d(128)\n",
    "        )\n",
    "        self.audio_fc = nn.Sequential(\n",
    "            nn.Linear(audio_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.BatchNorm1d(128)\n",
    "        )\n",
    "\n",
    "        # Cross-Modality Attention Mechanism\n",
    "        self.text_to_audio_attention = nn.Sequential(\n",
    "            nn.Linear(128, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        self.audio_to_text_attention = nn.Sequential(\n",
    "            nn.Linear(128, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        # Fusion Layer (Hybrid: concatenation + element-wise product)\n",
    "        self.fusion_fc = nn.Sequential(\n",
    "            nn.Linear(128 * 3, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.BatchNorm1d(64)\n",
    "        )\n",
    "\n",
    "        # Classifier\n",
    "        self.classifier = nn.Linear(64, num_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, text_embed, audio_embed):\n",
    "        # Feature extraction\n",
    "        text_features = self.text_fc(text_embed)\n",
    "        audio_features = self.audio_fc(audio_embed)\n",
    "\n",
    "        # Cross-modality attention\n",
    "        text_att_weights = self.text_to_audio_attention(audio_features)  # Attention from audio to text\n",
    "        audio_att_weights = self.audio_to_text_attention(text_features)  # Attention from text to audio\n",
    "\n",
    "        text_att_features = text_features * audio_att_weights  # Apply attention weights\n",
    "        audio_att_features = audio_features * text_att_weights  # Apply attention weights\n",
    "\n",
    "        # Fusion\n",
    "        concat_features = torch.cat((text_att_features, audio_att_features, text_features * audio_features), dim=1)\n",
    "        fused_features = self.fusion_fc(concat_features)\n",
    "\n",
    "        # Classification\n",
    "        y_logits = self.classifier(fused_features)\n",
    "        y_softmax = self.softmax(y_logits)\n",
    "\n",
    "        return y_logits, y_softmax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(y_pred, y_true):\n",
    "    class_weights = {cls: 1.0/count for cls, count in Counter(y_true).items()}\n",
    "    wa = balanced_accuracy_score(y_true, y_pred, sample_weight=[class_weights[cls] for cls in y_true])\n",
    "    ua = accuracy_score(y_true, y_pred)\n",
    "    return wa, ua"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, dataloader, optim, loss_fn, accuracy_fn):\n",
    "    train_loss = 0.0\n",
    "    train_wa = 0.0\n",
    "    train_ua = 0.0\n",
    "    y_true_ls = []\n",
    "    y_pred_ls = []\n",
    "    \n",
    "    model.train()\n",
    "    for batch, (text_embed, audio_embed, label) in enumerate(dataloader):\n",
    "        text_embed = text_embed.to(device)\n",
    "        audio_embed = audio_embed.to(device)\n",
    "        label = label.to(device)\n",
    "        output_logits, output_softmax = model(text_embed, audio_embed)\n",
    "        output_logits, output_softmax = output_logits.to(device), output_softmax.to(device)\n",
    "        y_preds = output_softmax.argmax(dim=1).to(device)\n",
    "        \n",
    "        wa, ua = calculate_accuracy(y_preds.cpu().numpy(), label.cpu().numpy())\n",
    "        y_true_ls.append(label.cpu().numpy())\n",
    "        y_pred_ls.append(y_preds.cpu().numpy())\n",
    "        loss = loss_fn(output_logits, label)\n",
    "        \n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        train_wa += wa\n",
    "        train_ua += ua\n",
    "        \n",
    "        # if batch % 20 == 0:\n",
    "        #     print(f\"\\tBatch {batch}: Train loss: {loss:.5f} | Train WA : {wa:.4f} | Train UA : {ua:.4f}\")\n",
    "        #     print(\"----------------------------------------\")\n",
    "        \n",
    "    train_loss /= len(dataloader)\n",
    "    train_wa /= len(dataloader)\n",
    "    train_ua /= len(dataloader)\n",
    "    print(f\"Total Train loss: {train_loss:.5f} | Total Train WA : {wa:.4f} | Total Train UA : {ua:.4f}\")\n",
    "    \n",
    "    return train_loss, train_wa, train_ua, y_true_ls, y_pred_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_step(model, dataloader, loss_fn, accuracy_fn):\n",
    "    eval_loss = 0.0\n",
    "    eval_wa = 0.0\n",
    "    eval_ua = 0.0\n",
    "    y_true_ls = []\n",
    "    y_pred_ls = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch, (text_embed, audio_embed, label) in enumerate(dataloader):\n",
    "            text_embed = text_embed.to(device)\n",
    "            audio_embed = audio_embed.to(device)\n",
    "            label = label.to(device)\n",
    "            output_logits, output_softmax = model(text_embed, audio_embed)\n",
    "            output_logits, output_softmax = output_logits.to(device), output_softmax.to(device)\n",
    "            y_preds = output_softmax.argmax(dim=1).to(device)\n",
    "            \n",
    "            wa, ua = calculate_accuracy(y_preds.cpu().numpy(), label.cpu().numpy())\n",
    "            y_true_ls.append(label.cpu().numpy())\n",
    "            y_pred_ls.append(y_preds.cpu().numpy())\n",
    "            loss = loss_fn(output_logits, label)\n",
    "\n",
    "            eval_loss += loss.item()\n",
    "            eval_wa += wa\n",
    "            eval_ua += ua\n",
    "\n",
    "            # if batch % 20 == 0:\n",
    "            #     print(f\"\\tBatch {batch}: Test loss: {loss:.5f} | Test WA : {wa:.4f} | Test UA : {ua:.4f}\")\n",
    "            #     print(\"----------------------------------------\")\n",
    "        \n",
    "        eval_loss /= len(dataloader)\n",
    "        eval_wa /= len(dataloader)\n",
    "        eval_ua /= len(dataloader)\n",
    "        print(f\"Total Test loss: {eval_loss:.5f} | Total Test WA: {eval_wa:.4f} | Total Test UA: {eval_ua:4f}\")\n",
    "        \n",
    "        return eval_loss, eval_wa, eval_ua, y_true_ls, y_pred_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = []\n",
    "train_loss_hist, train_wa_hist, train_ua_hist, val_loss_hist, val_wa_hist, val_ua_hist = [], [], [], [], [], []\n",
    "\n",
    "best_w = {}\n",
    "\n",
    "best_train_loss, best_val_loss = 10000, 10000\n",
    "best_wa, best_ua = 0.0, 0.0\n",
    "\n",
    "NUM_EPOCHS = 200\n",
    "\n",
    "model=HybridAttentionMMSER(num_classes=4)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Total Train loss: 1.14789 | Total Train WA : 0.6750 | Total Train UA : 0.6727\n",
      "Total Test loss: 1.32858 | Total Test WA: 0.4211 | Total Test UA: 0.396354\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 1\n",
      "Total Train loss: 0.91570 | Total Train WA : 0.6766 | Total Train UA : 0.6545\n",
      "Total Test loss: 1.33109 | Total Test WA: 0.4481 | Total Test UA: 0.444992\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 2\n",
      "Total Train loss: 0.80578 | Total Train WA : 0.7299 | Total Train UA : 0.7273\n",
      "Total Test loss: 1.27438 | Total Test WA: 0.4566 | Total Test UA: 0.459736\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 3\n",
      "Total Train loss: 0.75156 | Total Train WA : 0.8332 | Total Train UA : 0.8182\n",
      "Total Test loss: 1.27954 | Total Test WA: 0.4617 | Total Test UA: 0.495433\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 4\n",
      "Total Train loss: 0.69676 | Total Train WA : 0.7731 | Total Train UA : 0.7273\n",
      "Total Test loss: 1.52345 | Total Test WA: 0.4461 | Total Test UA: 0.432732\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 5\n",
      "Total Train loss: 0.66139 | Total Train WA : 0.8006 | Total Train UA : 0.8000\n",
      "Total Test loss: 1.43138 | Total Test WA: 0.4308 | Total Test UA: 0.456811\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 6\n",
      "Total Train loss: 0.65533 | Total Train WA : 0.7863 | Total Train UA : 0.7636\n",
      "Total Test loss: 1.44656 | Total Test WA: 0.4844 | Total Test UA: 0.492067\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 7\n",
      "Total Train loss: 0.63830 | Total Train WA : 0.7619 | Total Train UA : 0.7455\n",
      "Total Test loss: 1.41489 | Total Test WA: 0.4544 | Total Test UA: 0.464183\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 8\n",
      "Total Train loss: 0.62344 | Total Train WA : 0.6454 | Total Train UA : 0.6545\n",
      "Total Test loss: 1.54004 | Total Test WA: 0.4540 | Total Test UA: 0.461058\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 9\n",
      "Total Train loss: 0.60205 | Total Train WA : 0.7642 | Total Train UA : 0.7273\n",
      "Total Test loss: 1.44463 | Total Test WA: 0.4776 | Total Test UA: 0.497877\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 10\n",
      "Total Train loss: 0.56647 | Total Train WA : 0.8756 | Total Train UA : 0.8545\n",
      "Total Test loss: 1.45353 | Total Test WA: 0.4805 | Total Test UA: 0.485817\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 11\n",
      "Total Train loss: 0.55689 | Total Train WA : 0.7836 | Total Train UA : 0.7636\n",
      "Total Test loss: 1.67416 | Total Test WA: 0.4354 | Total Test UA: 0.435176\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 12\n",
      "Total Train loss: 0.57878 | Total Train WA : 0.6935 | Total Train UA : 0.6727\n",
      "Total Test loss: 1.50752 | Total Test WA: 0.4804 | Total Test UA: 0.488942\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 13\n",
      "Total Train loss: 0.55350 | Total Train WA : 0.8521 | Total Train UA : 0.8364\n",
      "Total Test loss: 1.56385 | Total Test WA: 0.4569 | Total Test UA: 0.469752\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 14\n",
      "Total Train loss: 0.53548 | Total Train WA : 0.7498 | Total Train UA : 0.7455\n",
      "Total Test loss: 1.71716 | Total Test WA: 0.4104 | Total Test UA: 0.442989\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 15\n",
      "Total Train loss: 0.52365 | Total Train WA : 0.9293 | Total Train UA : 0.9091\n",
      "Total Test loss: 1.54595 | Total Test WA: 0.4673 | Total Test UA: 0.488942\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 16\n",
      "Total Train loss: 0.48738 | Total Train WA : 0.8266 | Total Train UA : 0.8364\n",
      "Total Test loss: 1.47978 | Total Test WA: 0.4682 | Total Test UA: 0.490505\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 17\n",
      "Total Train loss: 0.53488 | Total Train WA : 0.7471 | Total Train UA : 0.6364\n",
      "Total Test loss: 1.62950 | Total Test WA: 0.4272 | Total Test UA: 0.461498\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 18\n",
      "Total Train loss: 0.48432 | Total Train WA : 0.8613 | Total Train UA : 0.8364\n",
      "Total Test loss: 1.70353 | Total Test WA: 0.4311 | Total Test UA: 0.453686\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 19\n",
      "Total Train loss: 0.50278 | Total Train WA : 0.7685 | Total Train UA : 0.7455\n",
      "Total Test loss: 1.63215 | Total Test WA: 0.4713 | Total Test UA: 0.492067\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 20\n",
      "Total Train loss: 0.48933 | Total Train WA : 0.7865 | Total Train UA : 0.7818\n",
      "Total Test loss: 1.65800 | Total Test WA: 0.4444 | Total Test UA: 0.466867\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 21\n",
      "Total Train loss: 0.46392 | Total Train WA : 0.7726 | Total Train UA : 0.7455\n",
      "Total Test loss: 1.77332 | Total Test WA: 0.4396 | Total Test UA: 0.449679\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 22\n",
      "Total Train loss: 0.46618 | Total Train WA : 0.8261 | Total Train UA : 0.7636\n",
      "Total Test loss: 1.59284 | Total Test WA: 0.4743 | Total Test UA: 0.493630\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 23\n",
      "Total Train loss: 0.44774 | Total Train WA : 0.8337 | Total Train UA : 0.8364\n",
      "Total Test loss: 1.64610 | Total Test WA: 0.4942 | Total Test UA: 0.500561\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 24\n",
      "Total Train loss: 0.43766 | Total Train WA : 0.9091 | Total Train UA : 0.8909\n",
      "Total Test loss: 1.74898 | Total Test WA: 0.4150 | Total Test UA: 0.453245\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 25\n",
      "Total Train loss: 0.41660 | Total Train WA : 0.8392 | Total Train UA : 0.8182\n",
      "Total Test loss: 2.01011 | Total Test WA: 0.4303 | Total Test UA: 0.434295\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 26\n",
      "Total Train loss: 0.43777 | Total Train WA : 0.8634 | Total Train UA : 0.8545\n",
      "Total Test loss: 1.74957 | Total Test WA: 0.4308 | Total Test UA: 0.442989\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 27\n",
      "Total Train loss: 0.43457 | Total Train WA : 0.9243 | Total Train UA : 0.9273\n",
      "Total Test loss: 1.93308 | Total Test WA: 0.4721 | Total Test UA: 0.493630\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 28\n",
      "Total Train loss: 0.42113 | Total Train WA : 0.8902 | Total Train UA : 0.8545\n",
      "Total Test loss: 1.72134 | Total Test WA: 0.4885 | Total Test UA: 0.499439\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 29\n",
      "Total Train loss: 0.41071 | Total Train WA : 0.8366 | Total Train UA : 0.8364\n",
      "Total Test loss: 1.98520 | Total Test WA: 0.4795 | Total Test UA: 0.477123\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 30\n",
      "Total Train loss: 0.40009 | Total Train WA : 0.8785 | Total Train UA : 0.8727\n",
      "Total Test loss: 1.69259 | Total Test WA: 0.4682 | Total Test UA: 0.493189\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 31\n",
      "Total Train loss: 0.35480 | Total Train WA : 0.8584 | Total Train UA : 0.8364\n",
      "Total Test loss: 1.69410 | Total Test WA: 0.4611 | Total Test UA: 0.483373\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 32\n",
      "Total Train loss: 0.33920 | Total Train WA : 0.8648 | Total Train UA : 0.8727\n",
      "Total Test loss: 1.72605 | Total Test WA: 0.4672 | Total Test UA: 0.488061\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 33\n",
      "Total Train loss: 0.32489 | Total Train WA : 0.8981 | Total Train UA : 0.8909\n",
      "Total Test loss: 1.73608 | Total Test WA: 0.4769 | Total Test UA: 0.494752\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 34\n",
      "Total Train loss: 0.32607 | Total Train WA : 0.9435 | Total Train UA : 0.9273\n",
      "Total Test loss: 1.72543 | Total Test WA: 0.4607 | Total Test UA: 0.480248\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 35\n",
      "Total Train loss: 0.32677 | Total Train WA : 0.9375 | Total Train UA : 0.9273\n",
      "Total Test loss: 1.67459 | Total Test WA: 0.4714 | Total Test UA: 0.494752\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 36\n",
      "Total Train loss: 0.30916 | Total Train WA : 0.8433 | Total Train UA : 0.8364\n",
      "Total Test loss: 1.69423 | Total Test WA: 0.4724 | Total Test UA: 0.495873\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 37\n",
      "Total Train loss: 0.31792 | Total Train WA : 0.8176 | Total Train UA : 0.7818\n",
      "Total Test loss: 1.77008 | Total Test WA: 0.4701 | Total Test UA: 0.488502\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 38\n",
      "Total Train loss: 0.31067 | Total Train WA : 0.8370 | Total Train UA : 0.8182\n",
      "Total Test loss: 1.79104 | Total Test WA: 0.4656 | Total Test UA: 0.486939\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 39\n",
      "Total Train loss: 0.30029 | Total Train WA : 0.8491 | Total Train UA : 0.8545\n",
      "Total Test loss: 1.70712 | Total Test WA: 0.4705 | Total Test UA: 0.491186\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 40\n",
      "Total Train loss: 0.30099 | Total Train WA : 0.9012 | Total Train UA : 0.8545\n",
      "Total Test loss: 1.76369 | Total Test WA: 0.4701 | Total Test UA: 0.488502\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 41\n",
      "Total Train loss: 0.28908 | Total Train WA : 0.9615 | Total Train UA : 0.9636\n",
      "Total Test loss: 1.79762 | Total Test WA: 0.4815 | Total Test UA: 0.495192\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 42\n",
      "Total Train loss: 0.30191 | Total Train WA : 0.8746 | Total Train UA : 0.8364\n",
      "Total Test loss: 1.71841 | Total Test WA: 0.4750 | Total Test UA: 0.491186\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 43\n",
      "Total Train loss: 0.30727 | Total Train WA : 0.9092 | Total Train UA : 0.9091\n",
      "Total Test loss: 1.73390 | Total Test WA: 0.4683 | Total Test UA: 0.486498\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 44\n",
      "Total Train loss: 0.30059 | Total Train WA : 0.8980 | Total Train UA : 0.8727\n",
      "Total Test loss: 1.83311 | Total Test WA: 0.4691 | Total Test UA: 0.483373\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 45\n",
      "Total Train loss: 0.28359 | Total Train WA : 0.9321 | Total Train UA : 0.9273\n",
      "Total Test loss: 1.77700 | Total Test WA: 0.4783 | Total Test UA: 0.497436\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 46\n",
      "Total Train loss: 0.28062 | Total Train WA : 0.9152 | Total Train UA : 0.9091\n",
      "Total Test loss: 1.76162 | Total Test WA: 0.4766 | Total Test UA: 0.501002\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 47\n",
      "Total Train loss: 0.27597 | Total Train WA : 0.8893 | Total Train UA : 0.8909\n",
      "Total Test loss: 1.76227 | Total Test WA: 0.4715 | Total Test UA: 0.491186\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 48\n",
      "Total Train loss: 0.28155 | Total Train WA : 0.9545 | Total Train UA : 0.9455\n",
      "Total Test loss: 1.75445 | Total Test WA: 0.4793 | Total Test UA: 0.496314\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 49\n",
      "Total Train loss: 0.28144 | Total Train WA : 0.9591 | Total Train UA : 0.9636\n",
      "Total Test loss: 1.73922 | Total Test WA: 0.4683 | Total Test UA: 0.488061\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 50\n",
      "Total Train loss: 0.27912 | Total Train WA : 0.9714 | Total Train UA : 0.9636\n",
      "Total Test loss: 1.75171 | Total Test WA: 0.4756 | Total Test UA: 0.497436\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 51\n",
      "Total Train loss: 0.27530 | Total Train WA : 0.9722 | Total Train UA : 0.9636\n",
      "Total Test loss: 1.78585 | Total Test WA: 0.4755 | Total Test UA: 0.494311\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 52\n",
      "Total Train loss: 0.27746 | Total Train WA : 0.9281 | Total Train UA : 0.9273\n",
      "Total Test loss: 1.78482 | Total Test WA: 0.4719 | Total Test UA: 0.488502\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 53\n",
      "Total Train loss: 0.27550 | Total Train WA : 0.8782 | Total Train UA : 0.8727\n",
      "Total Test loss: 1.81366 | Total Test WA: 0.4799 | Total Test UA: 0.497877\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 54\n",
      "Total Train loss: 0.27794 | Total Train WA : 0.8750 | Total Train UA : 0.8909\n",
      "Total Test loss: 1.79880 | Total Test WA: 0.4601 | Total Test UA: 0.477123\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 55\n",
      "Total Train loss: 0.26487 | Total Train WA : 0.8995 | Total Train UA : 0.8909\n",
      "Total Test loss: 1.78210 | Total Test WA: 0.4749 | Total Test UA: 0.494311\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 56\n",
      "Total Train loss: 0.25363 | Total Train WA : 0.9298 | Total Train UA : 0.9273\n",
      "Total Test loss: 1.79696 | Total Test WA: 0.4742 | Total Test UA: 0.497436\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 57\n",
      "Total Train loss: 0.26184 | Total Train WA : 0.8972 | Total Train UA : 0.8909\n",
      "Total Test loss: 1.83328 | Total Test WA: 0.4774 | Total Test UA: 0.493189\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 58\n",
      "Total Train loss: 0.27218 | Total Train WA : 0.8709 | Total Train UA : 0.8545\n",
      "Total Test loss: 1.80638 | Total Test WA: 0.4816 | Total Test UA: 0.497877\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 59\n",
      "Total Train loss: 0.25813 | Total Train WA : 0.9444 | Total Train UA : 0.9273\n",
      "Total Test loss: 1.83331 | Total Test WA: 0.4849 | Total Test UA: 0.501002\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 60\n",
      "Total Train loss: 0.25047 | Total Train WA : 0.8950 | Total Train UA : 0.8909\n",
      "Total Test loss: 1.82241 | Total Test WA: 0.4819 | Total Test UA: 0.499439\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 61\n",
      "Total Train loss: 0.24958 | Total Train WA : 0.9219 | Total Train UA : 0.9091\n",
      "Total Test loss: 1.80203 | Total Test WA: 0.4781 | Total Test UA: 0.494752\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 62\n",
      "Total Train loss: 0.25026 | Total Train WA : 0.8794 | Total Train UA : 0.8364\n",
      "Total Test loss: 1.81524 | Total Test WA: 0.4836 | Total Test UA: 0.499439\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 63\n",
      "Total Train loss: 0.24636 | Total Train WA : 0.9020 | Total Train UA : 0.8909\n",
      "Total Test loss: 1.82375 | Total Test WA: 0.4834 | Total Test UA: 0.499439\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 64\n",
      "Total Train loss: 0.26980 | Total Train WA : 0.8678 | Total Train UA : 0.8364\n",
      "Total Test loss: 1.85863 | Total Test WA: 0.4817 | Total Test UA: 0.497877\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 65\n",
      "Total Train loss: 0.25861 | Total Train WA : 0.9518 | Total Train UA : 0.9636\n",
      "Total Test loss: 1.85690 | Total Test WA: 0.4830 | Total Test UA: 0.499439\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 66\n",
      "Total Train loss: 0.24177 | Total Train WA : 0.8997 | Total Train UA : 0.8909\n",
      "Total Test loss: 1.80872 | Total Test WA: 0.4860 | Total Test UA: 0.504127\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 67\n",
      "Total Train loss: 0.23872 | Total Train WA : 0.9018 | Total Train UA : 0.8909\n",
      "Total Test loss: 1.79443 | Total Test WA: 0.4862 | Total Test UA: 0.502564\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 68\n",
      "Total Train loss: 0.24733 | Total Train WA : 0.9474 | Total Train UA : 0.9273\n",
      "Total Test loss: 1.79842 | Total Test WA: 0.4794 | Total Test UA: 0.496314\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 69\n",
      "Total Train loss: 0.24018 | Total Train WA : 0.9219 | Total Train UA : 0.9091\n",
      "Total Test loss: 1.85502 | Total Test WA: 0.4780 | Total Test UA: 0.493189\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 70\n",
      "Total Train loss: 0.25053 | Total Train WA : 0.8444 | Total Train UA : 0.8364\n",
      "Total Test loss: 1.80174 | Total Test WA: 0.4748 | Total Test UA: 0.493189\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 71\n",
      "Total Train loss: 0.25207 | Total Train WA : 0.8948 | Total Train UA : 0.8909\n",
      "Total Test loss: 1.85212 | Total Test WA: 0.4783 | Total Test UA: 0.493189\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 72\n",
      "Total Train loss: 0.23422 | Total Train WA : 0.9016 | Total Train UA : 0.8909\n",
      "Total Test loss: 1.82072 | Total Test WA: 0.4815 | Total Test UA: 0.499880\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 73\n",
      "Total Train loss: 0.24262 | Total Train WA : 0.8520 | Total Train UA : 0.8545\n",
      "Total Test loss: 1.80180 | Total Test WA: 0.4809 | Total Test UA: 0.497877\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 74\n",
      "Total Train loss: 0.23865 | Total Train WA : 0.9022 | Total Train UA : 0.9091\n",
      "Total Test loss: 1.84004 | Total Test WA: 0.4842 | Total Test UA: 0.503005\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 75\n",
      "Total Train loss: 0.24737 | Total Train WA : 0.9174 | Total Train UA : 0.9091\n",
      "Total Test loss: 1.82882 | Total Test WA: 0.4782 | Total Test UA: 0.494752\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 76\n",
      "Total Train loss: 0.24643 | Total Train WA : 0.9217 | Total Train UA : 0.8727\n",
      "Total Test loss: 1.82407 | Total Test WA: 0.4819 | Total Test UA: 0.497877\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 77\n",
      "Total Train loss: 0.24836 | Total Train WA : 0.9395 | Total Train UA : 0.9455\n",
      "Total Test loss: 1.82180 | Total Test WA: 0.4750 | Total Test UA: 0.490064\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 78\n",
      "Total Train loss: 0.24992 | Total Train WA : 0.9335 | Total Train UA : 0.9273\n",
      "Total Test loss: 1.83396 | Total Test WA: 0.4785 | Total Test UA: 0.494752\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 79\n",
      "Total Train loss: 0.24388 | Total Train WA : 0.9248 | Total Train UA : 0.9273\n",
      "Total Test loss: 1.84868 | Total Test WA: 0.4818 | Total Test UA: 0.497877\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 80\n",
      "Total Train loss: 0.24390 | Total Train WA : 0.9323 | Total Train UA : 0.9273\n",
      "Total Test loss: 1.83250 | Total Test WA: 0.4808 | Total Test UA: 0.497877\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 81\n",
      "Total Train loss: 0.23910 | Total Train WA : 0.8839 | Total Train UA : 0.8727\n",
      "Total Test loss: 1.84089 | Total Test WA: 0.4796 | Total Test UA: 0.496314\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 82\n",
      "Total Train loss: 0.24045 | Total Train WA : 0.8183 | Total Train UA : 0.8182\n",
      "Total Test loss: 1.83503 | Total Test WA: 0.4818 | Total Test UA: 0.497877\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 83\n",
      "Total Train loss: 0.24179 | Total Train WA : 0.9518 | Total Train UA : 0.9273\n",
      "Total Test loss: 1.85611 | Total Test WA: 0.4798 | Total Test UA: 0.494752\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 84\n",
      "Total Train loss: 0.24069 | Total Train WA : 0.9365 | Total Train UA : 0.9455\n",
      "Total Test loss: 1.83902 | Total Test WA: 0.4808 | Total Test UA: 0.497877\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 85\n",
      "Total Train loss: 0.24520 | Total Train WA : 0.9455 | Total Train UA : 0.9455\n",
      "Total Test loss: 1.83531 | Total Test WA: 0.4884 | Total Test UA: 0.506130\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 86\n",
      "Total Train loss: 0.22598 | Total Train WA : 0.9641 | Total Train UA : 0.9636\n",
      "Total Test loss: 1.82876 | Total Test WA: 0.4799 | Total Test UA: 0.496314\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 87\n",
      "Total Train loss: 0.23170 | Total Train WA : 0.9605 | Total Train UA : 0.9455\n",
      "Total Test loss: 1.86343 | Total Test WA: 0.4805 | Total Test UA: 0.496314\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 88\n",
      "Total Train loss: 0.25427 | Total Train WA : 0.8845 | Total Train UA : 0.8909\n",
      "Total Test loss: 1.81506 | Total Test WA: 0.4861 | Total Test UA: 0.504127\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 89\n",
      "Total Train loss: 0.25298 | Total Train WA : 0.9330 | Total Train UA : 0.9273\n",
      "Total Test loss: 1.86250 | Total Test WA: 0.4768 | Total Test UA: 0.491627\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 90\n",
      "Total Train loss: 0.23605 | Total Train WA : 0.8633 | Total Train UA : 0.8545\n",
      "Total Test loss: 1.85437 | Total Test WA: 0.4847 | Total Test UA: 0.501002\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 91\n",
      "Total Train loss: 0.24671 | Total Train WA : 0.8032 | Total Train UA : 0.8000\n",
      "Total Test loss: 1.84454 | Total Test WA: 0.4806 | Total Test UA: 0.496314\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 92\n",
      "Total Train loss: 0.24268 | Total Train WA : 0.9342 | Total Train UA : 0.9091\n",
      "Total Test loss: 1.84441 | Total Test WA: 0.4822 | Total Test UA: 0.497877\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 93\n",
      "Total Train loss: 0.22828 | Total Train WA : 0.9403 | Total Train UA : 0.9273\n",
      "Total Test loss: 1.83024 | Total Test WA: 0.4798 | Total Test UA: 0.496314\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 94\n",
      "Total Train loss: 0.25300 | Total Train WA : 0.9167 | Total Train UA : 0.9091\n",
      "Total Test loss: 1.81998 | Total Test WA: 0.4813 | Total Test UA: 0.497877\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 95\n",
      "Total Train loss: 0.23357 | Total Train WA : 0.9629 | Total Train UA : 0.9636\n",
      "Total Test loss: 1.83749 | Total Test WA: 0.4825 | Total Test UA: 0.499439\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 96\n",
      "Total Train loss: 0.24567 | Total Train WA : 0.9036 | Total Train UA : 0.8909\n",
      "Total Test loss: 1.86971 | Total Test WA: 0.4858 | Total Test UA: 0.503005\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 97\n",
      "Total Train loss: 0.24521 | Total Train WA : 0.9067 | Total Train UA : 0.8909\n",
      "Total Test loss: 1.85327 | Total Test WA: 0.4822 | Total Test UA: 0.499880\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 98\n",
      "Total Train loss: 0.24019 | Total Train WA : 0.8705 | Total Train UA : 0.8727\n",
      "Total Test loss: 1.85550 | Total Test WA: 0.4727 | Total Test UA: 0.491186\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 99\n",
      "Total Train loss: 0.24858 | Total Train WA : 0.8814 | Total Train UA : 0.8545\n",
      "Total Test loss: 1.87283 | Total Test WA: 0.4811 | Total Test UA: 0.496314\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 100\n",
      "Total Train loss: 0.23370 | Total Train WA : 0.8801 | Total Train UA : 0.8727\n",
      "Total Test loss: 1.82072 | Total Test WA: 0.4859 | Total Test UA: 0.504567\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 101\n",
      "Total Train loss: 0.24907 | Total Train WA : 0.8493 | Total Train UA : 0.8364\n",
      "Total Test loss: 1.84304 | Total Test WA: 0.4850 | Total Test UA: 0.503005\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 102\n",
      "Total Train loss: 0.24887 | Total Train WA : 0.9444 | Total Train UA : 0.9273\n",
      "Total Test loss: 1.85868 | Total Test WA: 0.4802 | Total Test UA: 0.496314\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 103\n",
      "Total Train loss: 0.23709 | Total Train WA : 0.8341 | Total Train UA : 0.8182\n",
      "Total Test loss: 1.85152 | Total Test WA: 0.4791 | Total Test UA: 0.494752\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 104\n",
      "Total Train loss: 0.23862 | Total Train WA : 0.9307 | Total Train UA : 0.9091\n",
      "Total Test loss: 1.86543 | Total Test WA: 0.4826 | Total Test UA: 0.497877\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 105\n",
      "Total Train loss: 0.23937 | Total Train WA : 0.9320 | Total Train UA : 0.9091\n",
      "Total Test loss: 1.84998 | Total Test WA: 0.4798 | Total Test UA: 0.494752\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 106\n",
      "Total Train loss: 0.24103 | Total Train WA : 0.9028 | Total Train UA : 0.8727\n",
      "Total Test loss: 1.81732 | Total Test WA: 0.4806 | Total Test UA: 0.496314\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 107\n",
      "Total Train loss: 0.24655 | Total Train WA : 0.9590 | Total Train UA : 0.9455\n",
      "Total Test loss: 1.83944 | Total Test WA: 0.4800 | Total Test UA: 0.496314\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 108\n",
      "Total Train loss: 0.24427 | Total Train WA : 0.9123 | Total Train UA : 0.9091\n",
      "Total Test loss: 1.84619 | Total Test WA: 0.4818 | Total Test UA: 0.499439\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 109\n",
      "Total Train loss: 0.23843 | Total Train WA : 0.9659 | Total Train UA : 0.9455\n",
      "Total Test loss: 1.88195 | Total Test WA: 0.4798 | Total Test UA: 0.494752\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 110\n",
      "Total Train loss: 0.25142 | Total Train WA : 0.9773 | Total Train UA : 0.9636\n",
      "Total Test loss: 1.87862 | Total Test WA: 0.4798 | Total Test UA: 0.496314\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 111\n",
      "Total Train loss: 0.24231 | Total Train WA : 0.9476 | Total Train UA : 0.9273\n",
      "Total Test loss: 1.84821 | Total Test WA: 0.4784 | Total Test UA: 0.494752\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 112\n",
      "Total Train loss: 0.24294 | Total Train WA : 0.9056 | Total Train UA : 0.9091\n",
      "Total Test loss: 1.83026 | Total Test WA: 0.4845 | Total Test UA: 0.501002\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 113\n",
      "Total Train loss: 0.24567 | Total Train WA : 0.9412 | Total Train UA : 0.9273\n",
      "Total Test loss: 1.86341 | Total Test WA: 0.4828 | Total Test UA: 0.499880\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 114\n",
      "Total Train loss: 0.23564 | Total Train WA : 0.8820 | Total Train UA : 0.8909\n",
      "Total Test loss: 1.86364 | Total Test WA: 0.4821 | Total Test UA: 0.496314\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 115\n",
      "Total Train loss: 0.23754 | Total Train WA : 0.9337 | Total Train UA : 0.9091\n",
      "Total Test loss: 1.85085 | Total Test WA: 0.4876 | Total Test UA: 0.504567\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 116\n",
      "Total Train loss: 0.24448 | Total Train WA : 0.8958 | Total Train UA : 0.8909\n",
      "Total Test loss: 1.86025 | Total Test WA: 0.4793 | Total Test UA: 0.496314\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 117\n",
      "Total Train loss: 0.23569 | Total Train WA : 0.9196 | Total Train UA : 0.8909\n",
      "Total Test loss: 1.87353 | Total Test WA: 0.4818 | Total Test UA: 0.496314\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 118\n",
      "Total Train loss: 0.23958 | Total Train WA : 0.9038 | Total Train UA : 0.9091\n",
      "Total Test loss: 1.81970 | Total Test WA: 0.4846 | Total Test UA: 0.502564\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 119\n",
      "Total Train loss: 0.24999 | Total Train WA : 0.8783 | Total Train UA : 0.8727\n",
      "Total Test loss: 1.84446 | Total Test WA: 0.4820 | Total Test UA: 0.497877\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 120\n",
      "Total Train loss: 0.25037 | Total Train WA : 0.8545 | Total Train UA : 0.8545\n",
      "Total Test loss: 1.82987 | Total Test WA: 0.4831 | Total Test UA: 0.499439\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 121\n",
      "Total Train loss: 0.22992 | Total Train WA : 0.9500 | Total Train UA : 0.9455\n",
      "Total Test loss: 1.86595 | Total Test WA: 0.4796 | Total Test UA: 0.496314\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 122\n",
      "Total Train loss: 0.24438 | Total Train WA : 0.9412 | Total Train UA : 0.9273\n",
      "Total Test loss: 1.84483 | Total Test WA: 0.4826 | Total Test UA: 0.499439\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 123\n",
      "Total Train loss: 0.23641 | Total Train WA : 0.8685 | Total Train UA : 0.8364\n",
      "Total Test loss: 1.83049 | Total Test WA: 0.4839 | Total Test UA: 0.501002\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 124\n",
      "Total Train loss: 0.23823 | Total Train WA : 0.9044 | Total Train UA : 0.8727\n",
      "Total Test loss: 1.83277 | Total Test WA: 0.4829 | Total Test UA: 0.499439\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 125\n",
      "Total Train loss: 0.26281 | Total Train WA : 0.8365 | Total Train UA : 0.8364\n",
      "Total Test loss: 1.85365 | Total Test WA: 0.4773 | Total Test UA: 0.493189\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 126\n",
      "Total Train loss: 0.22870 | Total Train WA : 0.9087 | Total Train UA : 0.9091\n",
      "Total Test loss: 1.81555 | Total Test WA: 0.4900 | Total Test UA: 0.507692\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 127\n",
      "Total Train loss: 0.23218 | Total Train WA : 0.9476 | Total Train UA : 0.9455\n",
      "Total Test loss: 1.81911 | Total Test WA: 0.4809 | Total Test UA: 0.497877\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 128\n",
      "Total Train loss: 0.24035 | Total Train WA : 0.9424 | Total Train UA : 0.9455\n",
      "Total Test loss: 1.82725 | Total Test WA: 0.4771 | Total Test UA: 0.493189\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 129\n",
      "Total Train loss: 0.23058 | Total Train WA : 0.9513 | Total Train UA : 0.9455\n",
      "Total Test loss: 1.84321 | Total Test WA: 0.4807 | Total Test UA: 0.494752\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 130\n",
      "Total Train loss: 0.24264 | Total Train WA : 0.9550 | Total Train UA : 0.9455\n",
      "Total Test loss: 1.80932 | Total Test WA: 0.4837 | Total Test UA: 0.501002\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 131\n",
      "Total Train loss: 0.24302 | Total Train WA : 0.8323 | Total Train UA : 0.8364\n",
      "Total Test loss: 1.81771 | Total Test WA: 0.4869 | Total Test UA: 0.504567\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 132\n",
      "Total Train loss: 0.25647 | Total Train WA : 0.9451 | Total Train UA : 0.9273\n",
      "Total Test loss: 1.83895 | Total Test WA: 0.4811 | Total Test UA: 0.496314\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 133\n",
      "Total Train loss: 0.24233 | Total Train WA : 0.9665 | Total Train UA : 0.9636\n",
      "Total Test loss: 1.86420 | Total Test WA: 0.4794 | Total Test UA: 0.496314\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 134\n",
      "Total Train loss: 0.24838 | Total Train WA : 0.9117 | Total Train UA : 0.8909\n",
      "Total Test loss: 1.87536 | Total Test WA: 0.4824 | Total Test UA: 0.497877\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 135\n",
      "Total Train loss: 0.23303 | Total Train WA : 0.9451 | Total Train UA : 0.9273\n",
      "Total Test loss: 1.85103 | Total Test WA: 0.4784 | Total Test UA: 0.494752\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 136\n",
      "Total Train loss: 0.24600 | Total Train WA : 0.8732 | Total Train UA : 0.8727\n",
      "Total Test loss: 1.83985 | Total Test WA: 0.4822 | Total Test UA: 0.499439\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 137\n",
      "Total Train loss: 0.24143 | Total Train WA : 0.9583 | Total Train UA : 0.9636\n",
      "Total Test loss: 1.82746 | Total Test WA: 0.4862 | Total Test UA: 0.502564\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 138\n",
      "Total Train loss: 0.23766 | Total Train WA : 0.8417 | Total Train UA : 0.8364\n",
      "Total Test loss: 1.89945 | Total Test WA: 0.4800 | Total Test UA: 0.496314\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 139\n",
      "Total Train loss: 0.23838 | Total Train WA : 0.8996 | Total Train UA : 0.8909\n",
      "Total Test loss: 1.82101 | Total Test WA: 0.4797 | Total Test UA: 0.497877\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 140\n",
      "Total Train loss: 0.23687 | Total Train WA : 0.8734 | Total Train UA : 0.8727\n",
      "Total Test loss: 1.82098 | Total Test WA: 0.4838 | Total Test UA: 0.501002\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 141\n",
      "Total Train loss: 0.23422 | Total Train WA : 0.8901 | Total Train UA : 0.8727\n",
      "Total Test loss: 1.84790 | Total Test WA: 0.4834 | Total Test UA: 0.497877\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 142\n",
      "Total Train loss: 0.23638 | Total Train WA : 0.9364 | Total Train UA : 0.9273\n",
      "Total Test loss: 1.85271 | Total Test WA: 0.4779 | Total Test UA: 0.493189\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 143\n",
      "Total Train loss: 0.23163 | Total Train WA : 0.8906 | Total Train UA : 0.8727\n",
      "Total Test loss: 1.82913 | Total Test WA: 0.4831 | Total Test UA: 0.501002\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 144\n",
      "Total Train loss: 0.25196 | Total Train WA : 0.8725 | Total Train UA : 0.8545\n",
      "Total Test loss: 1.81877 | Total Test WA: 0.4766 | Total Test UA: 0.493189\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 145\n",
      "Total Train loss: 0.24130 | Total Train WA : 0.9047 | Total Train UA : 0.8909\n",
      "Total Test loss: 1.83855 | Total Test WA: 0.4812 | Total Test UA: 0.496314\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 146\n",
      "Total Train loss: 0.24100 | Total Train WA : 0.9087 | Total Train UA : 0.9091\n",
      "Total Test loss: 1.88258 | Total Test WA: 0.4808 | Total Test UA: 0.496314\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 147\n",
      "Total Train loss: 0.23597 | Total Train WA : 0.9509 | Total Train UA : 0.9455\n",
      "Total Test loss: 1.84287 | Total Test WA: 0.4832 | Total Test UA: 0.501002\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 148\n",
      "Total Train loss: 0.23812 | Total Train WA : 0.9535 | Total Train UA : 0.9455\n",
      "Total Test loss: 1.84156 | Total Test WA: 0.4858 | Total Test UA: 0.503005\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 149\n",
      "Total Train loss: 0.23887 | Total Train WA : 0.9297 | Total Train UA : 0.9091\n",
      "Total Test loss: 1.85554 | Total Test WA: 0.4828 | Total Test UA: 0.497877\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 150\n",
      "Total Train loss: 0.24585 | Total Train WA : 0.8791 | Total Train UA : 0.8909\n",
      "Total Test loss: 1.88106 | Total Test WA: 0.4811 | Total Test UA: 0.497877\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 151\n",
      "Total Train loss: 0.25258 | Total Train WA : 0.8951 | Total Train UA : 0.8909\n",
      "Total Test loss: 1.85335 | Total Test WA: 0.4819 | Total Test UA: 0.497877\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 152\n",
      "Total Train loss: 0.23866 | Total Train WA : 0.9357 | Total Train UA : 0.9091\n",
      "Total Test loss: 1.85173 | Total Test WA: 0.4800 | Total Test UA: 0.496314\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 153\n",
      "Total Train loss: 0.25443 | Total Train WA : 0.8820 | Total Train UA : 0.8909\n",
      "Total Test loss: 1.84702 | Total Test WA: 0.4798 | Total Test UA: 0.496314\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 154\n",
      "Total Train loss: 0.25011 | Total Train WA : 0.9231 | Total Train UA : 0.9273\n",
      "Total Test loss: 1.82836 | Total Test WA: 0.4839 | Total Test UA: 0.499439\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 155\n",
      "Total Train loss: 0.23971 | Total Train WA : 0.9511 | Total Train UA : 0.9455\n",
      "Total Test loss: 1.83727 | Total Test WA: 0.4845 | Total Test UA: 0.501002\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 156\n",
      "Total Train loss: 0.24197 | Total Train WA : 0.8463 | Total Train UA : 0.8545\n",
      "Total Test loss: 1.84005 | Total Test WA: 0.4831 | Total Test UA: 0.499439\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 157\n",
      "Total Train loss: 0.23646 | Total Train WA : 0.9094 | Total Train UA : 0.9091\n",
      "Total Test loss: 1.82158 | Total Test WA: 0.4895 | Total Test UA: 0.507692\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 158\n",
      "Total Train loss: 0.24636 | Total Train WA : 0.8997 | Total Train UA : 0.9091\n",
      "Total Test loss: 1.84537 | Total Test WA: 0.4801 | Total Test UA: 0.496314\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 159\n",
      "Total Train loss: 0.23305 | Total Train WA : 0.9417 | Total Train UA : 0.9455\n",
      "Total Test loss: 1.83055 | Total Test WA: 0.4822 | Total Test UA: 0.499439\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 160\n",
      "Total Train loss: 0.23400 | Total Train WA : 0.8971 | Total Train UA : 0.8727\n",
      "Total Test loss: 1.81981 | Total Test WA: 0.4810 | Total Test UA: 0.498317\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 161\n",
      "Total Train loss: 0.23854 | Total Train WA : 0.8833 | Total Train UA : 0.8545\n",
      "Total Test loss: 1.90263 | Total Test WA: 0.4777 | Total Test UA: 0.493189\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 162\n",
      "Total Train loss: 0.24977 | Total Train WA : 0.9174 | Total Train UA : 0.9091\n",
      "Total Test loss: 1.84264 | Total Test WA: 0.4854 | Total Test UA: 0.503005\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 163\n",
      "Total Train loss: 0.23873 | Total Train WA : 0.8757 | Total Train UA : 0.8909\n",
      "Total Test loss: 1.81629 | Total Test WA: 0.4809 | Total Test UA: 0.496314\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 164\n",
      "Total Train loss: 0.24052 | Total Train WA : 0.9196 | Total Train UA : 0.9091\n",
      "Total Test loss: 1.84776 | Total Test WA: 0.4809 | Total Test UA: 0.497877\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 165\n",
      "Total Train loss: 0.23308 | Total Train WA : 0.9111 | Total Train UA : 0.8909\n",
      "Total Test loss: 1.84558 | Total Test WA: 0.4805 | Total Test UA: 0.496314\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 166\n",
      "Total Train loss: 0.24254 | Total Train WA : 0.9492 | Total Train UA : 0.9273\n",
      "Total Test loss: 1.86928 | Total Test WA: 0.4833 | Total Test UA: 0.497877\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 167\n",
      "Total Train loss: 0.24212 | Total Train WA : 0.9268 | Total Train UA : 0.9091\n",
      "Total Test loss: 1.83527 | Total Test WA: 0.4804 | Total Test UA: 0.496314\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 168\n",
      "Total Train loss: 0.23293 | Total Train WA : 0.9264 | Total Train UA : 0.9273\n",
      "Total Test loss: 1.82638 | Total Test WA: 0.4821 | Total Test UA: 0.499880\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 169\n",
      "Total Train loss: 0.23634 | Total Train WA : 0.9495 | Total Train UA : 0.9455\n",
      "Total Test loss: 1.84204 | Total Test WA: 0.4764 | Total Test UA: 0.495873\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 170\n",
      "Total Train loss: 0.24457 | Total Train WA : 0.8788 | Total Train UA : 0.8727\n",
      "Total Test loss: 1.83818 | Total Test WA: 0.4830 | Total Test UA: 0.499439\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 171\n",
      "Total Train loss: 0.23877 | Total Train WA : 0.9129 | Total Train UA : 0.9091\n",
      "Total Test loss: 1.83978 | Total Test WA: 0.4810 | Total Test UA: 0.497877\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 172\n",
      "Total Train loss: 0.24240 | Total Train WA : 0.8991 | Total Train UA : 0.8545\n",
      "Total Test loss: 1.84204 | Total Test WA: 0.4804 | Total Test UA: 0.496314\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 173\n",
      "Total Train loss: 0.24123 | Total Train WA : 0.9444 | Total Train UA : 0.9273\n",
      "Total Test loss: 1.82176 | Total Test WA: 0.4809 | Total Test UA: 0.497877\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 174\n",
      "Total Train loss: 0.24532 | Total Train WA : 0.8814 | Total Train UA : 0.8182\n",
      "Total Test loss: 1.82733 | Total Test WA: 0.4802 | Total Test UA: 0.496314\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 175\n",
      "Total Train loss: 0.24099 | Total Train WA : 0.8818 | Total Train UA : 0.8364\n",
      "Total Test loss: 1.85098 | Total Test WA: 0.4776 | Total Test UA: 0.491627\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 176\n",
      "Total Train loss: 0.25026 | Total Train WA : 0.8627 | Total Train UA : 0.8545\n",
      "Total Test loss: 1.84344 | Total Test WA: 0.4837 | Total Test UA: 0.499439\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 177\n",
      "Total Train loss: 0.23797 | Total Train WA : 0.9333 | Total Train UA : 0.9273\n",
      "Total Test loss: 1.84298 | Total Test WA: 0.4828 | Total Test UA: 0.499439\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 178\n",
      "Total Train loss: 0.23658 | Total Train WA : 0.9271 | Total Train UA : 0.9091\n",
      "Total Test loss: 1.82942 | Total Test WA: 0.4820 | Total Test UA: 0.499439\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 179\n",
      "Total Train loss: 0.24434 | Total Train WA : 0.9594 | Total Train UA : 0.9636\n",
      "Total Test loss: 1.85373 | Total Test WA: 0.4836 | Total Test UA: 0.499439\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 180\n",
      "Total Train loss: 0.24484 | Total Train WA : 0.8370 | Total Train UA : 0.8000\n",
      "Total Test loss: 1.82611 | Total Test WA: 0.4798 | Total Test UA: 0.496314\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 181\n",
      "Total Train loss: 0.25055 | Total Train WA : 0.8818 | Total Train UA : 0.8545\n",
      "Total Test loss: 1.85375 | Total Test WA: 0.4789 | Total Test UA: 0.494752\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 182\n",
      "Total Train loss: 0.24550 | Total Train WA : 0.9051 | Total Train UA : 0.9091\n",
      "Total Test loss: 1.86430 | Total Test WA: 0.4833 | Total Test UA: 0.499439\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 183\n",
      "Total Train loss: 0.23731 | Total Train WA : 0.9530 | Total Train UA : 0.9455\n",
      "Total Test loss: 1.86957 | Total Test WA: 0.4855 | Total Test UA: 0.503005\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 184\n",
      "Total Train loss: 0.23704 | Total Train WA : 0.8690 | Total Train UA : 0.8545\n",
      "Total Test loss: 1.83086 | Total Test WA: 0.4820 | Total Test UA: 0.497877\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 185\n",
      "Total Train loss: 0.23804 | Total Train WA : 0.9496 | Total Train UA : 0.9455\n",
      "Total Test loss: 1.85421 | Total Test WA: 0.4819 | Total Test UA: 0.499439\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 186\n",
      "Total Train loss: 0.24015 | Total Train WA : 0.8778 | Total Train UA : 0.8727\n",
      "Total Test loss: 1.82329 | Total Test WA: 0.4809 | Total Test UA: 0.497877\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 187\n",
      "Total Train loss: 0.23876 | Total Train WA : 0.9099 | Total Train UA : 0.9273\n",
      "Total Test loss: 1.84004 | Total Test WA: 0.4854 | Total Test UA: 0.503005\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 188\n",
      "Total Train loss: 0.23447 | Total Train WA : 0.9337 | Total Train UA : 0.9455\n",
      "Total Test loss: 1.84588 | Total Test WA: 0.4855 | Total Test UA: 0.502564\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 189\n",
      "Total Train loss: 0.25181 | Total Train WA : 0.9432 | Total Train UA : 0.9273\n",
      "Total Test loss: 1.84935 | Total Test WA: 0.4823 | Total Test UA: 0.499880\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 190\n",
      "Total Train loss: 0.24900 | Total Train WA : 0.8766 | Total Train UA : 0.8727\n",
      "Total Test loss: 1.83401 | Total Test WA: 0.4817 | Total Test UA: 0.497877\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 191\n",
      "Total Train loss: 0.23970 | Total Train WA : 0.8532 | Total Train UA : 0.8545\n",
      "Total Test loss: 1.87414 | Total Test WA: 0.4821 | Total Test UA: 0.496314\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 192\n",
      "Total Train loss: 0.24803 | Total Train WA : 0.9246 | Total Train UA : 0.9091\n",
      "Total Test loss: 1.82927 | Total Test WA: 0.4828 | Total Test UA: 0.499439\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 193\n",
      "Total Train loss: 0.23910 | Total Train WA : 0.9327 | Total Train UA : 0.9273\n",
      "Total Test loss: 1.83749 | Total Test WA: 0.4834 | Total Test UA: 0.501002\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 194\n",
      "Total Train loss: 0.23847 | Total Train WA : 0.9437 | Total Train UA : 0.9455\n",
      "Total Test loss: 1.81884 | Total Test WA: 0.4869 | Total Test UA: 0.504127\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 195\n",
      "Total Train loss: 0.23268 | Total Train WA : 0.9531 | Total Train UA : 0.9455\n",
      "Total Test loss: 1.83489 | Total Test WA: 0.4848 | Total Test UA: 0.503005\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 196\n",
      "Total Train loss: 0.24614 | Total Train WA : 0.8296 | Total Train UA : 0.8182\n",
      "Total Test loss: 1.82292 | Total Test WA: 0.4826 | Total Test UA: 0.499439\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 197\n",
      "Total Train loss: 0.24039 | Total Train WA : 0.8812 | Total Train UA : 0.8727\n",
      "Total Test loss: 1.85651 | Total Test WA: 0.4806 | Total Test UA: 0.496314\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 198\n",
      "Total Train loss: 0.24420 | Total Train WA : 0.9737 | Total Train UA : 0.9636\n",
      "Total Test loss: 1.84338 | Total Test WA: 0.4809 | Total Test UA: 0.497877\n",
      "\n",
      "==============================\n",
      "\n",
      "Epoch 199\n",
      "Total Train loss: 0.24551 | Total Train WA : 0.8553 | Total Train UA : 0.8545\n",
      "Total Test loss: 1.86231 | Total Test WA: 0.4810 | Total Test UA: 0.496314\n",
      "\n",
      "==============================\n",
      "\n",
      "Best WA:  0.456633908451985\n",
      "Best UA:  0.4597355769230769\n"
     ]
    }
   ],
   "source": [
    "# Train Multi-modal model\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(\"Epoch\", epoch)\n",
    "    train_loss, train_wa, train_ua, y_true_ls, y_pred_ls = train_step(model, train_dataloader, optimizer, criterion, calculate_accuracy)\n",
    "    val_loss, val_wa, val_ua, y_true_ls, y_pred_ls = eval_step(model, val_dataloader, criterion, calculate_accuracy)\n",
    "    lr_scheduler.step()\n",
    "    \n",
    "    epochs.append(epoch)\n",
    "    train_loss_hist.append(train_loss)\n",
    "    val_loss_hist.append(val_loss)\n",
    "    train_wa_hist.append(train_wa*100)\n",
    "    val_wa_hist.append(val_wa*100)\n",
    "    train_ua_hist.append(train_ua*100)\n",
    "    val_ua_hist.append(val_ua*100)\n",
    "    \n",
    "    if train_loss < best_train_loss and val_loss < best_val_loss:\n",
    "        best_train_loss, best_val_loss = train_loss, val_loss\n",
    "        best_wa, best_ua = val_wa, val_ua\n",
    "        torch.save(model.state_dict(), \"C:/Users/admin/Documents/Speech-Emotion_Recognition-2/saved_models/IEMOCAP_ENG_CMN_BERT_wav2vec.pt\")\n",
    "        best_w = model.state_dict()\n",
    "    \n",
    "    print(\"\\n==============================\\n\")\n",
    "print(\"Best WA: \", best_wa)\n",
    "print(\"Best UA: \", best_ua)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

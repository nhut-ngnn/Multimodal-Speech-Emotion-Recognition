{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch_geometric.data import Data\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.metrics import confusion_matrix, balanced_accuracy_score, accuracy_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph:\n",
      "Node Features (x): torch.Size([5, 128])\n",
      "Edge Index (edge_index): torch.Size([0])\n",
      "Labels (y): torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "def construct_graph(data_sample, threshold=0.8):\n",
    "    \"\"\"\n",
    "    Constructs a graph for a single data sample from IEMOCAP.\n",
    "    \n",
    "    Args:\n",
    "        data_sample: A dictionary containing 'audio_embed' and 'label'.\n",
    "        threshold: Similarity threshold for connecting nodes based on embeddings.\n",
    "    \n",
    "    Returns:\n",
    "        A PyG Data object representing the graph.\n",
    "    \"\"\"\n",
    "    audio_embeds = data_sample['audio_embed']  \n",
    "    labels = data_sample['label']              \n",
    "    num_nodes = len(audio_embeds)\n",
    "    \n",
    "    x = torch.tensor(audio_embeds, dtype=torch.float)\n",
    "    \n",
    "    edge_index = []\n",
    "    \n",
    "    for i in range(num_nodes):\n",
    "        for j in range(num_nodes):\n",
    "            if i != j:\n",
    "                sim = np.dot(audio_embeds[i], audio_embeds[j]) / (\n",
    "                    np.linalg.norm(audio_embeds[i]) * np.linalg.norm(audio_embeds[j])\n",
    "                )\n",
    "                if sim >= threshold:\n",
    "                    edge_index.append([i, j])\n",
    "    \n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous() \n",
    "    \n",
    "    y = torch.tensor(labels, dtype=torch.long)\n",
    "    \n",
    "    graph = Data(x=x, edge_index=edge_index, y=y)\n",
    "    return graph\n",
    "\n",
    "sample_data = {\n",
    "    'audio_embed': np.random.rand(5, 128),  \n",
    "    'label': [0, 1, 0, 2, 1],             \n",
    "}\n",
    "\n",
    "graph = construct_graph(sample_data)\n",
    "\n",
    "print(\"Graph:\")\n",
    "print(\"Node Features (x):\", graph.x.shape)  # (num_nodes, embed_dim)\n",
    "print(\"Edge Index (edge_index):\", graph.edge_index.shape)  # (2, num_edges)\n",
    "print(\"Labels (y):\", graph.y.shape)  # (num_nodes,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "train_metadata = \"C:/Users/admin/Documents/Speech-Emotion_Recognition-2/features/IEMOCAP_ECAPA_train.pkl\"\n",
    "val_metadata = \"C:/Users/admin/Documents/Speech-Emotion_Recognition-2/features/IEMOCAP_ECAPA_train.pkl\"\n",
    "# train_dataset = Customized_Dataset(train_metadata)\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "# val_dataset = Customized_Dataset(val_metadata)\n",
    "# val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjacency Matrix Shape: torch.Size([5, 5])\n",
      "Node Features Shape: torch.Size([5, 16])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def create_adj_matrix(num_nodes):\n",
    "    \"\"\"\n",
    "    Create an adjacency matrix for a fully connected graph of num_nodes.\n",
    "    \"\"\"\n",
    "    # Fully connected graph adjacency matrix\n",
    "    adj_matrix = np.ones((num_nodes, num_nodes)) - np.eye(num_nodes)  # Remove self-loops\n",
    "    return adj_matrix\n",
    "\n",
    "def create_node_features(num_nodes, feature_dim):\n",
    "    \"\"\"\n",
    "    Create random node features for a graph.\n",
    "    \"\"\"\n",
    "    # Randomly initialize node features\n",
    "    return np.random.rand(num_nodes, feature_dim)\n",
    "\n",
    "# Example usage for a dataset\n",
    "num_samples = 10  # Number of graphs in the dataset\n",
    "num_nodes = 5     # Nodes per graph\n",
    "feature_dim = 16  # Feature dimension per node\n",
    "\n",
    "adj_matrices = []\n",
    "node_features = []\n",
    "\n",
    "for _ in range(num_samples):\n",
    "    # Create adjacency matrix\n",
    "    adj_matrices.append(create_adj_matrix(num_nodes))\n",
    "\n",
    "    # Create node features\n",
    "    node_features.append(create_node_features(num_nodes, feature_dim))\n",
    "\n",
    "# Convert to PyTorch tensors if needed\n",
    "adj_matrices = [torch.tensor(adj, dtype=torch.float) for adj in adj_matrices]\n",
    "node_features = [torch.tensor(features, dtype=torch.float) for features in node_features]\n",
    "\n",
    "# Verify shapes\n",
    "print(\"Adjacency Matrix Shape:\", adj_matrices[0].shape)  \n",
    "print(\"Node Features Shape:\", node_features[0].shape)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

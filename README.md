# Multi-modal with Late Fusion for Speech Emotion Recognition</h1>

> Please press ‚≠ê button and/or cite papers if you feel helpful.

<p align="center">
<img src="https://img.shields.io/github/stars/nhut-ngnn/Multimodal-Speech-Emotion-Recognition">
<img src="https://img.shields.io/github/stars/nhut-ngnn/Multimodal-Speech-Emotion-Recognition">
<img src="https://img.shields.io/github/watchers/nhut-ngnn/Multimodal-Speech-Emotion-Recognition">
</p>

<p align="center">
<img src="https://img.shields.io/badge/Last%20updated%20on-23.11.2024-brightgreen?style=for-the-badge">
<img src="https://img.shields.io/badge/Written%20by-Nguyen%20Minh%20Nhut-pink?style=for-the-badge"> 
</p>

## Table of Contents

- [Abstract](#Abstract)
- [Usage](#Usage)
  - [Dataset](#dataset)
  - [Clone this repository](#clone-this-repository)
  - [Create Conda Enviroment and Install Requirement](#create-conda-enviroment-and-install-requirement)
- [Contact](#Contact)
## Usage
### Dataset
In this study, we use voice dataset from IEMOCAP. 

<a href="https://sail.usc.edu/iemocap/">Download in here</a>
### Clone this repository
```python
git clone "https://github.com/nhut-ngnn/Multimodal-Speech-Emotion-Recognition.git"
```


## Contact
For any information, please contact the main author:

Nhut Minh Nguyen at FPT University, Vietnam

Email: <link>minhnhut.ngnn@gmail.com </link>

GitHub: <link>https://github.com/nhut-ngnn</link>